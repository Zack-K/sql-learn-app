[
  {
    "day": 1,
    "title": "SOLで「手続き」を書く",
    "goal": "自然言語で書かれた手順をSOL風の手続きに落とす。",
    "question": "**【🔰 最初に：SOL（疑似言語）とは？】**<br>本ドリルでは、実際のSQL（`SELECT * FROM ...`）ではなく、「アルゴリズム的な思考」を鍛えるために **SOL（Sort of Like SQL: SQLのような疑似コード）** という架空のスクリプト言語を使用します。<br>SQLが「どんなデータが欲しいか（結果）」を書く宣言型言語であるのに対し、SOLは「データをどうやって取り出し、変換するか（手順）」を書く手続き型言語（PythonやJavaScriptのような書き方）です。これによって、データベースの裏側で動いている『処理の順序（パイプライン）』を意識する訓練を行います。<br>わからない処理があれば、「Python csv 読み込み forループ」などで調べるのが近道です。<br><hr>**【シナリオ】**<br>あなたはデータエンジニアとして、毎日送られてくる顧客のCSVファイル（`users.csv`）をシステムに取り込むデータ抽出（Extract）バッチを作成することになりました。<br><br>**【課題】**<br>以下の抽出処理を、疑似コード（SOL）を用いた1つの関数（手続き）として記述してください。<br>1. 指定されたファイルパスのCSVファイルを開く<br>2. 1行目（ヘッダー行）はデータではないので読み込みをスキップする<br>3. 残りの全レコードをループし、1行ずつ後続の処理（`process_row`等）に渡す",
    "schema": [
      {
        "table_name": "users.csv",
        "columns": [
          "id",
          "name",
          "email"
        ],
        "mock_data": [
          {
            "id": 1,
            "name": "Alice",
            "email": "alice@example.com"
          },
          {
            "id": 2,
            "name": "Bob",
            "email": "bob@example.com"
          }
        ]
      }
    ],
    "example_answer": "function process_csv(filepath) {\n    file = open(filepath)\n    file.skip_header()\n    for row in file:\n        process_row(row)\n}",
    "explanation": "ETLの入り口となる「抽出（Extract）」の基本的な考え方です。ファイルを行ごとにループ処理することはデータ処理の基本のキです。"
  },
  {
    "day": 2,
    "title": "条件分岐（if）の練習",
    "goal": "ETLでよくある条件分岐をSOLで表現する。",
    "question": "**【シナリオ】**<br>ECサイトの決済システムから送られてきたトランザクションデータ（`transactions`）には、決済失敗によるマイナス金額や、システムエラーによる未来日付の異常データが混ざっています。これらをデータウェアハウスに入れる前にクレンジング（浄化）する必要があります。<br><br>**【課題】**<br>取引レコードの配列を受け取り、保存処理（`save`等）を行う関数をSOLで記述してください。ただし、保存の前に以下のデータ品質チェック（条件分岐）を追加してください。<br>1. `amount`（金額）が0以下のレコードは保存せずスキップする<br>2. `transaction_date`（取引日）が実行日（今日）より未来のレコードは、スキップした上でエラーログ（`log_error`等）を出力する",
    "schema": [
      {
        "table_name": "transactions",
        "columns": [
          "transaction_id",
          "user_id",
          "amount",
          "transaction_date"
        ],
        "mock_data": [
          {
            "transaction_id": 101,
            "user_id": 1,
            "amount": 1500,
            "transaction_date": "2023-10-01"
          },
          {
            "transaction_id": 102,
            "user_id": 2,
            "amount": -500,
            "transaction_date": "2023-10-02"
          },
          {
            "transaction_id": 103,
            "user_id": 1,
            "amount": 2000,
            "transaction_date": "2099-12-31"
          }
        ]
      }
    ],
    "example_answer": "function process_transactions(records) {\n    for record in records:\n        if record.amount <= 0:\n            continue // 除外\n        if record.transaction_date > today():\n            log_error('未来日エラー: ' + record.id)\n            continue\n        save(record)\n}",
    "explanation": "条件分岐は「データのクレンジング」そのものです。不要なデータを弾き、異常値に対してアクション（ログ出力）を取る処理は実務で頻出します。"
  },
  {
    "day": 3,
    "title": "繰り返し処理（loop）の練習",
    "goal": "レコード配列をループしながら加工する。",
    "question": "**【シナリオ】**<br>経営層から「昨日発生した日本円（JPY）での売上総額を大至急知りたい」という依頼が来ました。データベースから抽出した全売上レコード（`sales`）の配列を使って、プログラム上で要件を満たす合計値を算出する必要があります。<br><br>**【課題】**<br>売上レコードの配列を引数として受け取り、以下の条件を満たす金額の合計値を返すアルゴリズムをSOLで書いてください。<br>1. 配列をループして `amount` を足し上げる<br>2. ただし、`currency` が 'JPY' のレコードの金額のみを合計の対象とする",
    "schema": [
      {
        "table_name": "sales",
        "columns": [
          "sale_id",
          "store_id",
          "amount",
          "currency",
          "created_at"
        ],
        "mock_data": [
          {
            "sale_id": 1,
            "store_id": 10,
            "amount": 5000,
            "currency": "JPY",
            "created_at": "2023-10-01"
          },
          {
            "sale_id": 2,
            "store_id": 10,
            "amount": 100,
            "currency": "USD",
            "created_at": "2023-10-01"
          },
          {
            "sale_id": 3,
            "store_id": 11,
            "amount": 3000,
            "currency": "JPY",
            "created_at": "2023-10-02"
          }
        ]
      }
    ],
    "example_answer": "function calculate_jpy_total(sales_records) {\n    total = 0\n    for record in sales_records:\n        if record.currency == 'JPY':\n            total = total + record.amount\n    return total\n}",
    "explanation": "配列のループと累積用の変数（totalなど）の組み合わせは、SQLのSUM関数の裏側の動きです。"
  },
  {
    "day": 4,
    "title": "フィルタリングとマッピング",
    "goal": "「WHERE」と「SELECT」のイメージをSOLで書く。",
    "question": "**【シナリオ】**<br>マーケティングチームから「配送が完了した注文のIDと金額の一覧リスト（CSV）が欲しい」と依頼されました。システムから取り出した生の注文データ（`orders`）には、まだ配送中（pending）のものや、リストには不要な顧客ID（customer_id）などの個人情報が含まれています。<br><br>**【課題】**<br>生の注文レコードの配列を受け取り、以下の加工を行った「新しい配列」を返す処理をSOLで書いてください。<br>1. `status` が 'completed' のレコードだけに絞り込む（フィルタリング）<br>2. 絞り込んだ結果から、`order_id` と `amount` だけを持つ新しいレコード（オブジェクト）を生成して配列に詰める（マッピング）",
    "schema": [
      {
        "table_name": "orders",
        "columns": [
          "order_id",
          "customer_id",
          "status",
          "amount",
          "ordered_at"
        ],
        "mock_data": [
          {
            "order_id": 1,
            "customer_id": 1,
            "status": "completed",
            "amount": 1000,
            "ordered_at": "2023-10-01"
          },
          {
            "order_id": 2,
            "customer_id": 2,
            "status": "pending",
            "amount": 500,
            "ordered_at": "2023-10-02"
          },
          {
            "order_id": 3,
            "customer_id": 1,
            "status": "completed",
            "amount": 2000,
            "ordered_at": "2023-10-03"
          }
        ]
      }
    ],
    "example_answer": "function filter_and_map_orders(orders) {\n    result = []\n    for order in orders:\n        if order.status == 'completed':\n            new_record = {\n                'order_id': order.order_id,\n                'amount': order.amount\n            }\n            result.push(new_record)\n    return result\n}",
    "explanation": "WHEREで絞り込み、SELECTで必要なカラムだけを抽出する処理は、配列では「Filter」と「Map」の概念に相当します。"
  },
  {
    "day": 5,
    "title": "グルーピング（単純版）",
    "goal": "GROUP BYの前段階として、グループ分けのループを考える。",
    "question": "**【シナリオ】**<br>営業部から「顧客ごとの累計購入金額を出して、優良顧客のリストを作りたい」と言われました。生の購入履歴データ（`purchases`）では同じ顧客が何度も登場するため、顧客ID（customer_id）を軸に金額を集計（グルーピング）する必要があります。<br><br>**【課題】**<br>`purchases` 配列を受け取り、`customer_id` ごとに `amount` を合計するアルゴリズムをSOLで書いてください。結果は `(customer_id, total_amount)` というキーを持つ新しいオブジェクトの配列として返す手続きにまとめてください。",
    "schema": [
      {
        "table_name": "purchases",
        "columns": [
          "purchase_id",
          "customer_id",
          "amount",
          "item_name"
        ],
        "mock_data": [
          {
            "purchase_id": 1,
            "customer_id": 1,
            "amount": 1200,
            "item_name": "Book"
          },
          {
            "purchase_id": 2,
            "customer_id": 2,
            "amount": 800,
            "item_name": "Pen"
          },
          {
            "purchase_id": 3,
            "customer_id": 1,
            "amount": 300,
            "item_name": "Notebook"
          }
        ]
      }
    ],
    "example_answer": "function group_by_customer(purchases) {\n    group_map = {}\n    for p in purchases:\n        if not group_map.has(p.customer_id):\n            group_map[p.customer_id] = 0\n        group_map[p.customer_id] += p.amount\n\n    result = []\n    for customer_id, total in group_map:\n        result.push({'customer_id': customer_id, 'total_amount': total})\n    return result\n}",
    "explanation": "SQLのGROUP BYの実態の一つである「Hash Aggregation」に相当する処理です。キーごとに集計用の枠（ハッシュマップ）を作ることで実現します。"
  },
  {
    "day": 6,
    "title": "ジョインの思考",
    "goal": "JOINをループと条件で分解して記述する。",
    "question": "**【シナリオ】**<br>注文データ（`orders`）には誰が買ったかというID（customer_id）しかありません。分析チームから「注文データに顧客の名前（name）をくっつけてほしい」と依頼されました。また、システムのバグで稀に「存在しない顧客ID」宛の注文が混在していることが発覚したため、それらを検知する必要があります。<br><br>**【課題】**<br>注文レコードの配列と、顧客マスタ（`customers`）の配列の2つを受け取ります。これらを「customer_id」で内部結合（INNER JOIN）するアルゴリズムを書いてください。さらに結合時、「顧客マスタに存在しない注文」を発見した場合は、結合結果には含めずエラーログに出す（参照整合性エラーの検知）処理も含めてください。",
    "schema": [
      {
        "table_name": "orders",
        "columns": [
          "order_id",
          "customer_id",
          "amount"
        ],
        "mock_data": [
          {
            "order_id": 1,
            "customer_id": 1,
            "status": "completed",
            "amount": 1000,
            "ordered_at": "2023-10-01"
          },
          {
            "order_id": 2,
            "customer_id": 2,
            "status": "pending",
            "amount": 500,
            "ordered_at": "2023-10-02"
          },
          {
            "order_id": 3,
            "customer_id": 1,
            "status": "completed",
            "amount": 2000,
            "ordered_at": "2023-10-03"
          }
        ]
      },
      {
        "table_name": "customers",
        "columns": [
          "customer_id",
          "name",
          "email"
        ],
        "mock_data": [
          {
            "customer_id": 1,
            "name": "Alice",
            "email": "alice@example.com"
          },
          {
            "customer_id": 2,
            "name": "Bob",
            "email": "bob@example.com"
          }
        ]
      }
    ],
    "example_answer": "function join_orders_customers(orders, customers) {\n    // 顧客マスタを検索しやすいようにMap化（Hash Joinの準備）\n    customer_map = {}\n    for c in customers:\n        customer_map[c.customer_id] = c\n\n    result = []\n    for o in orders:\n        if customer_map.has(o.customer_id):\n            c = customer_map[o.customer_id]\n            result.push({'order_id': o.order_id, 'name': c.name, 'amount': o.amount})\n        else:\n            log_error('顧客不明: ' + o.order_id)\n    return result\n}",
    "explanation": "データベースが実際に背後で行っている「Hash Join」の動きに似ています。また、マスタにないIDを検知する思考はパイプラインの品質保証上とても重要です。"
  },
  {
    "day": 7,
    "title": "実務ETLタスク1本を書いてみる",
    "goal": "ここまでの要素をつなげて1本のETLパイプラインをSOLで書く。",
    "question": "**【シナリオ】**<br>長年動いていた古い売上集計スクリプトを、あなたがモダンな形式でリファクタリングすることになりました。抽出、クレンジング、結合、集計という流れを持つ、典型的な日次ETLバッチ処理をひとつ作り上げます。<br><br>**【課題】**<br>以下の処理全体を、順次実行する1つの手続き（`run_daily_etl` 等）として記述してください。<br>1. `orders.csv` と `customers.csv` をそれぞれ読み込む<br>2. 注文データのうち、ステータスが 'completed' のものだけを残すようフィルタ<br>3. 注文と顧客マスタを結合する<br>4. 顧客ごとの売上金額を合計する<br>5. 最終結果を `customer_sales.csv` として書き出す",
    "schema": [
      {
        "table_name": "orders.csv",
        "columns": [
          "order_id",
          "customer_id",
          "status",
          "amount"
        ],
        "mock_data": [
          {
            "order_id": 1,
            "customer_id": 1,
            "status": "completed",
            "amount": 1000
          },
          {
            "order_id": 2,
            "customer_id": 2,
            "status": "pending",
            "amount": 500
          },
          {
            "order_id": 3,
            "customer_id": 99,
            "status": "completed",
            "amount": 2000
          }
        ]
      },
      {
        "table_name": "customers.csv",
        "columns": [
          "customer_id",
          "name"
        ],
        "mock_data": [
          {
            "customer_id": 1,
            "name": "Alice"
          },
          {
            "customer_id": 2,
            "name": "Bob"
          }
        ]
      },
      {
        "table_name": "[出力] customer_sales.csv",
        "columns": [
          "customer_id",
          "name",
          "total_amount"
        ]
      }
    ],
    "example_answer": "function run_daily_etl() {\n    orders = read_csv('orders.csv')\n    customers = read_csv('customers.csv')\n\n    valid_orders = filter(orders, o => o.status == 'completed')\n    joined_data = join_orders_customers(valid_orders, customers)\n    aggr_data = group_by_customer(joined_data)\n\n    write_csv('customer_sales.csv', aggr_data)\n}",
    "explanation": "これが「ETLスクリプト」の原型です。しかし、実際の業務ではこの関数が大きくなりすぎると保守ができなくなるため、ここから分割やツール（dbtなど）への移行が必要になります。"
  },
  {
    "day": 8,
    "title": "パイプラインのステップ分割",
    "goal": "Day7の処理を「タスク/ステップ」単位に分割して定義する。",
    "question": "**【シナリオ】**<br>Day7で作ったETL関数は動作しますが、コードが長すぎて「テストがしづらい」「途中で失敗したときの再実行が面倒」といった課題が出てきました。Airflow等のモダンなツールを導入するため、処理の塊（関心事）ごとに小さく切り分ける設計に直します。<br><br>**【課題】**<br>Day7で書いた1つの巨大な処理を「抽出(Extract)」「変換(Transform)」「ロード(Load)」に該当する 3〜5個の独立したステップ（関数）に分割してください。メインの手続きからは、それらの小さな関数を順番に呼び出すだけの構成に書き換えてください。",
    "schema": [
      {
        "table_name": "orders.csv",
        "columns": [
          "order_id",
          "customer_id",
          "status",
          "amount"
        ],
        "mock_data": [
          {
            "order_id": 1,
            "customer_id": 1,
            "status": "completed",
            "amount": 1000
          },
          {
            "order_id": 2,
            "customer_id": 2,
            "status": "pending",
            "amount": 500
          },
          {
            "order_id": 3,
            "customer_id": 99,
            "status": "completed",
            "amount": 2000
          }
        ]
      },
      {
        "table_name": "customers.csv",
        "columns": [
          "customer_id",
          "name"
        ],
        "mock_data": [
          {
            "customer_id": 1,
            "name": "Alice"
          },
          {
            "customer_id": 2,
            "name": "Bob"
          }
        ]
      }
    ],
    "example_answer": "function extract_data() { return [read_csv('orders.csv'), read_csv('customers.csv')] }\nfunction transform_data(orders, customers) { /* フィルタ・結合・集計処理 */ return aggr_data }\nfunction load_data(data) { write_csv('customer_sales.csv', data) }\n\n// メインプロセス\nfunction main_pipeline() {\n    [orders, customers] = extract_data()\n    transformed = transform_data(orders, customers)\n    load_data(transformed)\n}",
    "explanation": "役割ごとに処理を分割する（関心の分離）ことで、テストがしやすくなり、Airflowなどのオーケストレーションツールでタスクを管理しやすくなります。"
  },
  {
    "day": 9,
    "title": "依存関係の記述",
    "goal": "「このステップはAとBが終わってから」という依存（DAG）を表現する。",
    "question": "**【シナリオ】**<br>処理ステップを分割できましたが、一部の処理（例：顧客APIからの抽出と、注文DBからの抽出）は互いに無関係なので、同時に（並行して）実行すれば処理時間を短縮できます。一方で「変換（Transform）」は両方の抽出が終わるまで待つ必要があります。<br><br>**【課題】**<br>与えられた4つのタスク (`extract_orders`, `extract_customers`, `transform`, `load`) について、並列実行可能なものと、直列で待つ必要があるものをコメントで明示しつつ、全体をコントロールする1つの手続きとして記述してください。",
    "schema": [
      {
        "table_name": "Task: extract_orders",
        "columns": [
          "(Output: Raw Orders Array)"
        ]
      },
      {
        "table_name": "Task: extract_customers",
        "columns": [
          "(Output: Raw Customers Array)"
        ]
      },
      {
        "table_name": "Task: transform",
        "columns": [
          "(Input: Orders, Customers) -> (Output: Transformed)"
        ]
      }
    ],
    "example_answer": "function run_dag() {\n    // 【並列実行可能】お互いに依存していないため同時に動かせる\n    future_orders = execute_async(extract_orders)\n    future_customers = execute_async(extract_customers)\n\n    // 【直列実行】上の2つの完了を待つ必要がある\n    wait_all([future_orders, future_customers])\n    orders_data = future_orders.result()\n    customers_data = future_customers.result()\n\n    // 【直列実行】transformの完了を待ってloadを実行\n    transformed_data = transform(orders_data, customers_data)\n    load(transformed_data)\n}",
    "explanation": "これがAirflowやDagsterなどのオーケストレータにおけるDAG（有向非巡回グラフ）の考え方です。"
  },
  {
    "day": 10,
    "title": "エラー処理とリトライの思考",
    "goal": "ETLの実務で重要な「失敗時の挙動」をSOLで書く。",
    "question": "**【シナリオ】**<br>夜間のデータ抽出バッチが「相手先のAPIサーバーの一時的な瞬断」によって稀に失敗し、翌朝にデータエンジニアが手動で再実行する羽目になっています。一時的な通信エラーであれば、少し待ってから再度リトライすれば成功するはずです。<br><br>**【課題】**<br>APIからの抽出タスク（`extract_from_api`）を実行し、もし失敗（エラー）になったら最大3回までリトライするアルゴリズムをSOLで書いてください。もし3回連続で失敗した場合は、手には負えない障害とみなし、タスク全体を失敗扱いに（エラーを投げて）終了するようにしてください。",
    "schema": [
      {
        "table_name": "API Endpoint",
        "columns": [
          "GET /api/v1/sales_data"
        ],
        "mock_data": [
          {
            "GET /api/v1/sales_data": "Returns daily sales JSON array"
          }
        ]
      }
    ],
    "example_answer": "function robust_extract() {\n    max_retries = 3\n    for attempt in 1..max_retries:\n        try:\n            data = extract_from_api()\n            return data // 成功したらリトライ終了\n        catch (error):\n            log('アクセス失敗: 回数=' + attempt)\n            if attempt == max_retries:\n                throw new Error('API抽出に完全に失敗しました。パイプラインを停止します。')\n            sleep(1000) // 1秒待ってリトライ\n}",
    "explanation": "ネットワークやAPIは「必ず一時的に失敗する」前提で組むのがデータエンジニアの鉄則です。"
  },
  {
    "day": 11,
    "title": "スケジューリングの擬似表現",
    "goal": "「毎日0時に実行」「前日のデータだけ対象」などをアルゴリズム化する。",
    "question": "**【シナリオ】**<br>夜間バッチをクーロン（cron）等で定期起動する設定をしました。しかし、バッチの処理対象は「バッチが動いた日」ではなく「その前日1日分」のデータである必要があります。また、コスト削減のため「休日」はバッチを動かしたくないというビジネス要件もあります。<br><br>**【課題】**<br>「実行された日時（execution_time）」を引数として受け取る関数を定義してください。その中で、「実行時刻が 01:00 の場合のみ動作する」「処理対象日を前日に設定する」「対象日が休日（`is_holiday`等がTrue）ならスキップする」という制御ロジックをSOLで書いてください。",
    "schema": [
      {
        "table_name": "sales (Partitioned by Date)",
        "columns": [
          "sale_id",
          "amount",
          "sale_date (YYYY-MM-DD)"
        ],
        "mock_data": [
          {
            "sale_id": 1,
            "amount": 1500,
            "sale_date (YYYY-MM-DD)": "2023-10-01"
          },
          {
            "sale_id": 2,
            "amount": 2000,
            "sale_date (YYYY-MM-DD)": "2023-10-02"
          }
        ]
      },
      {
        "table_name": "holidays_master",
        "columns": [
          "holiday_date",
          "description"
        ],
        "mock_data": [
          {
            "holiday_date": "2023-10-09",
            "description": "Sports Day"
          }
        ]
      }
    ],
    "example_answer": "function scheduled_job(execution_time) {\n    if time_of(execution_time) != '01:00':\n        return // 指定時間以外は何もしない\n\n    target_date = date_of(execution_time) - 1_day\n\n    if is_holiday(target_date):\n        log('休日のため処理をスキップ')\n        return\n\n    process_sales_for_date(target_date)\n}",
    "explanation": "実行日時を変数で受け取れるように設計しておくことで、過去データの再実行（バックフィル）が容易になります。これは実運用の最重要ポイントです。"
  },
  {
    "day": 12,
    "title": "dbtっぽいモデル依存の表現",
    "goal": "「モデルAが終わったらB」といったdbtの参照（ref）をSOLで表す。",
    "question": "**【シナリオ】**<br>データウェアハウス（DWH）内に複数のSQLテーブル（モデル）ができました。生のソースを磨いた staging モデル（`stg_orders`, `stg_customers`）、それらを結合・集計した fact モデル（`fct_orders`）や dimension モデル（`dim_customer`）があります。これらを正しい順序で構築（ビルド）し直す必要があります。<br><br>**【課題】**<br>4つのモデルに対して、依存関係に従って実行順序を決めるアルゴリズムを書いてください。`fct_orders` は `stg_orders` と `stg_customers` の両方に依存しています。`dim_customer` は `stg_customers` にのみ依存しています。",
    "schema": [
      {
        "table_name": "ソース (Raw)",
        "columns": [
          "raw_orders",
          "raw_customers"
        ]
      },
      {
        "table_name": "変換層 (Models)",
        "columns": [
          "stg_orders",
          "stg_customers",
          "fct_orders",
          "dim_customer"
        ]
      }
    ],
    "example_answer": "function dbt_run_simulation() {\n    // Layer 1: ソースに依存（互いに独立）\n    run_model('stg_orders')\n    run_model('stg_customers')\n\n    // Layer 2: stagingの完了を待って実行（順不同）\n    wait_all_layer1()\n    run_model('fct_orders')      // uses stg_orders, stg_customers\n    run_model('dim_customer')    // uses stg_customers\n}",
    "explanation": "dbtは `{{ ref('stg_orders') }}` と書くだけで、この実行順序（DAG）を自動で計算し、並列実行してくれます。"
  },
  {
    "day": 13,
    "title": "変更時の再実行戦略",
    "goal": "「あるモデルだけ変更されたとき、どこまで再実行するか」を考える。",
    "question": "**【シナリオ】**<br>あなたは `stg_orders` テーブルの抽出ロジック（SQL）を少し修正しました。この変更をテストするためにパイプライン全体を再実行すると、膨大なコンピュートコストと時間がかかってしまいます。変更したモデルと、**その変更の影響を受ける下流のモデルだけ**を特定して再実行する賢い仕組みが必要です。<br><br>**【課題】**<br>Day12のDAG構造（モデル同士の依存リスト）が与えられたとき、「`stg_orders` が変更された場合に、再実行が必要となるモデルのリスト（配列）を特定して返す」アルゴリズムをSOLで書いてください。",
    "schema": [
      {
        "table_name": "DAG Dependencies",
        "columns": [
          "fct_orders -> [stg_orders, stg_customers]",
          "dim_customer -> [stg_customers]"
        ]
      }
    ],
    "example_answer": "function find_downstream_models(changed_model, all_deps) {\n    to_run = [changed_model]\n    for model in all_deps:\n        // 変更されたモデルに依存しているモデルがあれば追加する\n        if changed_model in model.depends_on:\n            to_run.push(model.name)\n            // 再帰的にその下流も追加するロジックが必要\n    return to_run\n}\n// 結果: ['stg_orders', 'fct_orders']\n// dim_customerはstg_ordersに依存していないため再実行不要",
    "explanation": "これはdbtの `dbt run --select stg_orders+` というコマンド（下流モデルも含める）の裏側で動いているロジックです。全量実行を避けてコストを抑える考え方です。"
  },
  {
    "day": 14,
    "title": "データテストの思考（dbtの核心）",
    "goal": "変換処理の直後に、結果の品質を保証するロジックを挟む。",
    "question": "**【シナリオ】**<br>ETLバッチが「成功」と表示されていても、実はソースデータがおかしくて「顧客IDが空っぽ」「売上がマイナス」といった異常データがDWHに入り込んでいることがあり、分析チームからクレームが来ました。処理後に、データそのものの「品質（状態）」をテストする防御壁が必要です。<br><br>**【課題】**<br>生成した `fct_orders` のデータ配列に対し、「`order_id` が一意（重複なし）であること」「`amount` がNULL（空）でないこと」をチェックし、もし1件でも違反があればエラーを投げてパイプライン全体を止めるアサーション（テスト）処理を追記してください。",
    "schema": [
      {
        "table_name": "fct_orders (検証対象)",
        "columns": [
          "order_id",
          "customer_id",
          "amount",
          "status"
        ],
        "mock_data": [
          {
            "order_id": "O1",
            "customer_id": 1,
            "amount": 1000,
            "status": "completed"
          },
          {
            "order_id": "O1",
            "customer_id": 1,
            "amount": 500,
            "status": "pending"
          },
          {
            "order_id": "O2",
            "customer_id": 2,
            "amount": null,
            "status": "completed"
          }
        ]
      }
    ],
    "example_answer": "function test_fct_orders(fct_data) {\n    seen_ids = {}\n    for row in fct_data:\n        // 非NULLテスト\n        if row.amount is NULL:\n            throw Error('NULL Amount found!')\n        \n        // 一意性（Unique）テスト\n        if seen_ids.has(row.order_id):\n            throw Error('Duplicate order_id: ' + row.order_id)\n        seen_ids[row.order_id] = true\n\n    return 'テスト通過'\n}",
    "explanation": "「ただ変換する」のではなく「変換結果をシステムにアサート（検証）させる」。これがモダンなデータモデリングにおける最大の防御壁です。"
  },
  {
    "day": 15,
    "title": "インクリメンタル（差分）更新と冪等性",
    "goal": "効率的で、何度実行してもデータが重複しない更新処理を書く。",
    "question": "**【シナリオ】**<br>毎回全データを洗い替えると時間がかかりすぎるため、今日変更があったデータだけを更新する「差分更新」に切り替えます。しかし、リトライなどでパイプラインが2回続けて走った際に、データが二重登録されてしまう問題が起きています。これを防ぐ『冪等性（べきとうせい：何度実行しても結果が同じになる性質）』を持たせなければなりません。<br><br>**【課題】**<br>昨日の既存データ（DWH）と今日の新データ（API等）を比較し、「新規のキー（ID）なら追加（INSERT）」「既存のキー（ID）なら最新の内容で上書き（UPDATE）」するアルゴリズム（UPSERT/MERGE処理）を書いてください。",
    "schema": [
      {
        "table_name": "dest_table (既存データ)",
        "columns": [
          "id",
          "val",
          "updated_at"
        ],
        "mock_data": [
          {
            "id": 1,
            "val": "A",
            "updated_at": "2023-10-01 10:00:00"
          },
          {
            "id": 2,
            "val": "B",
            "updated_at": "2023-10-01 11:00:00"
          }
        ]
      },
      {
        "table_name": "new_data (今日の抽出データ)",
        "columns": [
          "id",
          "val",
          "updated_at"
        ],
        "mock_data": [
          {
            "id": 2,
            "val": "B_updated",
            "updated_at": "2023-10-02 10:00:00"
          },
          {
            "id": 3,
            "val": "C",
            "updated_at": "2023-10-02 10:05:00"
          }
        ]
      }
    ],
    "example_answer": "function incremental_merge(existing_table, new_data) {\n    existing_map = map_by_id(existing_table)\n    \n    for row in new_data:\n        if existing_map.has(row.id):\n            // 既存にあれば最新値で上書き（UPDATE）\n            update_record(existing_table, row)\n        else:\n            // なければ新規追加（INSERT）\n            insert_record(existing_table, row)\n}",
    "explanation": "この「MERGE（UPSERT）」の仕組みがあることで、ETLが途中で失敗し再実行（リトライ）されてもデータが二重に登録されません。この性質を『冪等性（べきとうせい）』と呼びます。"
  },
  {
    "day": 16,
    "title": "半構造化データ（JSON・配列）の展開",
    "goal": "階層を持つデータをループで掘り下げ、平坦な構造に変換（Flatten）する。",
    "question": "**【シナリオ】**<br>WebアクセスログなどによくあるJSONデータは、「1人のユーザーが、複数の商品（配列）を含む」といった入れ子（階層）構造になっています。これをBIツールで分析しやすくするためには、RDBで扱える「平坦（フラット）」な表形式に直す（行を爆発させる）必要があります。<br><br>**【課題】**<br>`[{user: 'A', items: ['apple', 'banana']}, {user: 'B', items: ['orange']}]` のような入れ子のJSON入力配列を引数として受け取り、`(user, item)` という一次元の平坦なテーブル用レコードの配列に変換して返す処理（Flatten）をSOLで書いてください。",
    "schema": [
      {
        "table_name": "raw_json_events (入力)",
        "columns": [
          "user",
          "items (Array)"
        ],
        "mock_data": [
          {
            "user": "A",
            "items (Array)": "['apple', 'banana']"
          },
          {
            "user": "B",
            "items (Array)": "['orange']"
          }
        ]
      },
      {
        "table_name": "flattened_events (出力)",
        "columns": [
          "user",
          "item"
        ]
      }
    ],
    "example_answer": "function flatten_json_array(json_data) {\n    flat_records = []\n    for obj in json_data:\n        user_id = obj.user\n        // 内側の配列をもう一度ループする\n        for item_name in obj.items:\n            flat_records.push({'user': user_id, 'item': item_name})\n    return flat_records\n}",
    "explanation": "SnowflakeなどのDWHでは、JSONの配列を`FLATTEN`という関数等で一気に展開（行の爆発）させます。その中身はこのような二重ループのアルゴリズムです。"
  },
  {
    "day": 17,
    "title": "総合プロジェクト：パイプライン設計",
    "goal": "全16日間の知識を使って、一連のパイプラインのアーキテクチャを書く",
    "question": "**【シナリオ】**<br>あなたはデータ基盤のリードエンジニアに着任しました。最初の仕事は「Webサーバーから送られてくる生のJSONアクセスログを、分析用の整ったFactテーブルに変換し、冪等性を持ってDWHにロードする」というデータ基盤の設計です。<br><br>**【課題】**<br>Day1〜16の知識を総動員し、「抽出と生ロード → 展開(Flatten) → 変換(JOIN/集約) → データテスト(一意性等) → 差分マージ(UPSERT)」の一連のパイプライン処理を呼び出すメインとなる手続き群を書いてください。<br>さらに、各ステップが実務においてどのツール群（例：Airflow, Fivetran, dbt, Snowflake等）の役割に該当するかをコメントで注釈してください。",
    "schema": [
      {
        "table_name": "Source",
        "columns": [
          "Web Server JSON Logs (storage)"
        ]
      },
      {
        "table_name": "Data Warehouse Tables",
        "columns": [
          "raw_logs",
          "stg_logs",
          "fct_visits",
          "prod_fct_visits"
        ]
      }
    ],
    "example_answer": "function run_daily_log_pipeline() {\n    // [Airflow(Spark/Fivetran領域)] データの抽出と生のロード\n    raw_logs = extract_api_logs(target_date)\n    load_to_s3(raw_logs)\n\n    // [Snowflake + dbt領域] データモデル化\n    stg_logs = execute_flatten(raw_logs) // Arrayの展開\n    fct_visits = build_dbt_model('fct_visits', deps=[stg_logs]) // SQLで変換\n\n    // [dbt Test領域] 品質保証\n    test_uniqueness(fct_visits, 'session_id')\n\n    // [Snowflake領域] 冪等性を持ったインクリメンタルMerge\n    merge_into_table('prod_fct_visits', fct_visits)\n}",
    "explanation": "最終日お疲れ様でした！実務でのデータシステム構築は、まさにこの手続き群をモダンな特化ツールたちに分担させていく作業そのものです。"
  }
]